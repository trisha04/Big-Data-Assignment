1.Various sources of big data are:
Public—4-5 terabytes generated by new york stock exchange daily
Social network profiles-More than 7 petabytes,i.e 240 billion photos generated by facebook
Around 10 petabytes generated by geneology site Ancestory.com
18.5 petabytes generated by Internet Archieve
Around 30 petabytes are generated by large hadron collider every year
Social influencers-centric sites like Apple’s App Store, Amazon, ZDNet, etc. 
Software as a Service (SaaS) and cloud applications—Systems like Salesforce.com, Netsuite, SuccessFactors, etc.
 all represent data that’s already in the Cloud but is difficult to move and merge with internal data.
 
 2.3 Vs of Big Data
 Volume-That is the data size.Sometimes the same data is re-evaluated with multiple angles and even though the original data 
 is the same and this creates explosion of the data. The big volume indeed represents Big Data.
 There is exponential growth in the data storage as the data is now more than text data.
 Velocity-It refers to the data sources.The data movement is now almost real time and the update window 
 has reduced to fractions of the seconds. This high velocity data represent Big Data.Earlier yesterday's data was recent data.
 Now the technology has moved to a fast pace.Radio and news are the sources telecasting live news.Social media is 
 also used to share or discuss the latest happenings.
 Variety-Data can be stored in multiple format.Sometimes the data is not even in the traditional format as we assume,
 it may be in the form of video, SMS, pdf etc
 It is the need of the organization to arrange it and make it meaningful. 
 It will be easy to do so if we have data in the same format, however it is not the case most of the time. 
 The real world have data in many different formats and that is the challenge we need to overcome with the Big Data.
 This variety of the data represent  represent Big Data.
 
 
 Horizontal Scaling and vertical scaling
 Horizontal Scaling - also referred to as "scale-out" is basically the addition of more machines 
 or setting up a cluster or a distributed environment for our software system
 Vertical Scaling - also referred to as "scale-up" approach is an attempt to increase the capacity of a single machine :
 by adding more processing power by adding more storage More memory etc 
 
 Hadoop is a framework that allows for the distributed processing of large data
 sets across clusters of computers using simple programming models.
a.Fault detection and recovery : Since HDFS includes a large number of commodity hardware, 
failure of components is frequent. Therefore HDFS should have mechanisms for quick and automatic fault detection and recovery.

b.Huge datasets : HDFS should have hundreds of nodes per cluster to manage the applications having huge datasets.

c.Hardware at data : A requested task can be done efficiently, when the computation takes place near the data. 
Especially where huge datasets are involved, it reduces the network traffic and increases the throughput.
